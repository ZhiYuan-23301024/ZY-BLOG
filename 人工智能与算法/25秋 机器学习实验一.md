---
title: BJTU 软件学院机器学习实验一
categories:
  - 人工智能与算法
date: 2025-11-08
tags:
  - ML
---
## 前置知识

| 特征         | 线性回归 (Linear Regression)        | 逻辑回归 (Logistic Regression)          |
| ---------- | ------------------------------- | ----------------------------------- |
| ​**核心任务**​ | ​**回归**​：预测连续的数值                | ​**分类**​：预测样本所属的类别（通常是二分类）          |
| ​**输出形式**​ | 连续的实数（任意值）                      | 介于0和1之间的概率值                         |
| ​**核心原理**​ | 拟合自变量（特征）与因变量（目标）之间的**线性关系**​   | 在线性回归结果上套用 ​**Sigmoid 函数**，将输出映射为概率 |
| ​**损失函数**​ | ​**均方误差 (MSE)​**，最小化预测值与真实值的平方差 | ​**对数损失（交叉熵损失）​**，衡量预测概率与真实标签的差异    |
| ​**典型应用**​ | 房价预测、销售额趋势分析、股价预测               | 垃圾邮件识别、疾病诊断预测、广告点击率预估               |

### 原理

**线性回归**的目标是找到一条线性函数（或高维空间中的超平面），使得所有数据点到该直线的**距离之和最小**。它通过**最小二乘法**或**梯度下降法**来优化参数，最小化均方误差损失函数。在线性回归中，每个特征的权重（系数）有直观的解释，代表了当其他特征不变时，该特征每增加一个单位，预测值会变化多少。

**逻辑回归**虽然名字里有“回归”，但解决的是分类问题，特别是二分类问题。它首先计算特征的线性组合（`z = β₀ + β₁x₁ + ... + βₙxₙ`），然后将这个线性结果 `z`输入到 ​**Sigmoid 函数**​（也称为逻辑函数）中。Sigmoid函数能够将任何实数“压缩”到(0,1)区间，这个输出值就被解释为属于正类的概率。模型参数通常通过**最大似然估计**和**梯度下降法**进行优化，以最大程度地使预测的概率分布接近真实的数据分布。

## 线性回归实验

线性回归=预测函数＋损失策略+权重调整

这个函数到底是个什么样的函数属于事先设定的超参数
是不可以通过学习得到的，而是通过特征工程进行映射

> [!NOTE]
> 比如
> $$
> y=w_1 x~和~ y=w_1x^2
> $$
> 这个过程中训练过程不在乎函数形式，只在乎特征处理过后的线性函数权重
> 

### 假设预测函数

奥卡姆剃刀原则，从最符合直觉的角度出发
$$
{y} = \mathbf{w} \mathbf{x} + b
$$
$$
\mathbf{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_n \end{bmatrix}
$$
$$
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} 
$$
代码实现(初始化各权重为1，假定各个特征分量线性作用到结果，因此不进行特征工程),支持批量数据也支持单个样本

```python
n = 13  # 特征数量

# 初始化权重和偏置
W = np.zeros((1, n))  # 权重矩阵，形状 (1, n)
b = np.array([0.0])    # 偏置项，初始化为0，形状 (1,)

def predict(X, W, b):
    """
    Args:
        X: 特征输入，可以是单个样本（一维数组，形状 (n,1)）或多个样本（二维数组，形状 (m, n)）
        W: 权重矩阵，形状 (1, n)
        b: 偏置项，形状 (1,)
    Returns:
        预测值。单个样本返回标量，多个样本返回一维数组 (m,)
    """
    # 确保 X 至少是二维的，例如 [n_features] 变为 [1, n_features]
    X_reshaped = np.atleast_2d(X)  # 处理单个样本输入
    
    # 矩阵乘法：X_reshaped 形状 (m, n), W.T 形状 (n, 1) -> 结果形状 (m, 1)
    # 加上偏置 b（利用广播机制），最后压平为一维数组 (m,)
    predictions = np.dot(X_reshaped, W.T).flatten() + b
    
    # 返回一维数组,维数为样本数
    return predictions      # 返回一维数组
```

### 设计损失函数

线性回归通常使用均方误差作为损失函数
$$
J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\mathbf{w}}(\mathbf{x}^{(i)}) - y^{(i)})^2
$$
- `m`：样本数量
- `h_w(x⁽ⁱ⁾)`：第i个样本的预测值
- `y⁽ⁱ⁾`：第i个样本的真实值
- `w`：权重参数矩阵

代码实现对预测值数组进行loss计算


```python
def compute_mse_loss(y_pred, y):

    """计算均方误差损失"""

    m = len(y_pred)  # 样本数量

    # 计算所有样本的误差平方和

    total_squared_error = np.sum((y_pred - y) ** 2)

    # 计算平均损失

    mse = total_squared_error / m

    return mse
```

### 权重优化方法

#### 梯度下降

| 方法            | 原理                 | 优点        | 缺点         | 适用场景           |
| ------------- | ------------------ | --------- | ---------- | -------------- |
| ​**批量梯度下降**​  | 使用全部数据计算梯度         | 稳定收敛，方向准确 | 计算慢，内存要求高  | 小数据集，精确优化      |
| ​**随机梯度下降**​  | 每次使用1个样本计算梯度       | 速度快，可在线学习 | 震荡大，收敛不稳定  | 大数据集，实时学习      |
| ​**小批量梯度下降**​ | 每次使用mini-batch计算梯度 | 平衡速度与稳定性  | 需要调batch大小 | ​**最常用**，大多数场景 |

507条数据属于小数据集，因此使用批量梯度下降

代码实现

```python
def batch_gradient_descent(X, y, learning_rate, n_iterations):
    """
    批量梯度下降实现
    """
    m, n = X.shape
    w = np.zeros(n)  # 初始化权重
    b = 0            # 初始化偏置
    loss_history = []
    
    for i in range(n_iterations):
        # 计算预测值
        y_pred = X @ w + b
        
        # 计算误差
        errors = y_pred - y
        
        # 计算梯度
        dw = (1/m) * (X.T @ errors)
        db = (1/m) * np.sum(errors)
        
        # 更新参数
        w = w - learning_rate * dw
        b = b - learning_rate * db
        
        # 计算并记录损失
        loss = np.mean(errors ** 2)
        loss_history.append(loss)
        
        # 每100次迭代打印进度
        if i % 100 == 0:
            print(f"Iteration {i}: Loss = {loss:.4f}")
    
    return w, b, loss_history
```

### 完整手动代码实现

在完成上面三步设计以后可以进行完整的手动代码编写
```python
import pandas as pd

import numpy as np

import matplotlib as plt

from sklearn.discriminant_analysis import StandardScaler

from sklearn.model_selection import train_test_split

plt.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体为黑体

plt.rcParams['axes.unicode_minus'] = False # 解决负号'-'显示为方块的问题[1,3](@ref)

# 导入并预处理实验数据

def load_and_prepare_data(csv_file_path="housing-data.csv"):

    """

    修正后的CSV数据加载和特征矩阵准备函数

    修复了数据标准化顺序问题

    """

    # 1. 读取数据

    data = pd.read_csv(csv_file_path)

    print(f"原始数据形状: {data.shape}")

    # 2. 处理缺失值

    print("缺失值统计:")

    print(data.isnull().sum())

    # 删除缺失值

    data_clean = data.dropna()

    print(f"清理后数据形状: {data_clean.shape}")

    # 3. 分离特征和目标（假设最后一列是目标变量）

    X = data_clean.iloc[:, :-1]  # 所有特征列

    y = data_clean.iloc[:, -1]   # 目标列

    print(f"特征矩阵形状: {X.shape}")

    print(f"目标向量形状: {y.shape}")

    # 4. 先分割数据集

    X_train, X_test, y_train, y_test = train_test_split(

        X, y, test_size=0.2, random_state=42, stratify=None

    )

    # 5. 特征标准化（只在训练集上拟合）

    scaler = StandardScaler()

    X_train_scaled = scaler.fit_transform(X_train)  # 拟合训练集并转换

    X_test_scaled = scaler.transform(X_test)       # 用训练集的统计量转换测试集

    print(f"训练集形状: {X_train_scaled.shape}, 测试集形状: {X_test_scaled.shape}")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

  

# 预测函数

def predict(X_train, W, b):

    """

    Args:

        X_train: 特征输入，可以是单个样本（一维数组，形状 (n,1)）或多个样本（二维数组，形状 (m, n)）

        W: 权重矩阵，形状 (1, n)

        b: 偏置项，形状 (1,)

    Returns:

        预测值。单个样本返回标量，多个样本返回一维数组 (m,)

    """

    # 确保 X 至少是二维的，例如 [n_features] 变为 [1, n_features]

    X_reshaped = np.atleast_2d(X_train)  # 处理单个样本输入

    # 矩阵乘法：X_reshaped 形状 (m, n), W.T 形状 (n, 1) -> 结果形状 (m, 1)

    # 加上偏置 b（利用广播机制），最后压平为一维数组 (m,)

    predictions = np.dot(X_reshaped, W.T).flatten() + b

    # 返回一维数组,维数为样本数

    return predictions      # 返回一维数组

  

# 损失函数

def compute_mse_loss(y_pred, y):

    """计算均方误差损失"""

    m = len(y_pred)  # 样本数量

    # 计算所有样本的误差平方和

    total_squared_error = np.sum((y_pred - y) ** 2)

    # 计算平均损失

    mse = total_squared_error / m

    return mse

  

# 优化策略

def batch_gradient_descent(X_train, y_train, learning_rate, n_iterations):

    """

    批量梯度下降实现 - 使用已定义的predict和compute_mse_loss函数

    """

    m, n = X_train.shape

    w = np.zeros((1, n))  # 初始化权重，形状改为(1, n)以匹配predict函数的W形状要求

    b = 0                 # 初始化偏置

    loss_history = []

    for i in range(n_iterations):

        # 计算预测值

        y_pred = predict(X_train, w, b)

        # 计算误差

        errors = y_pred - y_train

        # 计算梯度

        dw = (2/m) * np.dot(X_train.T, errors)  # 乘以2因为MSE的导数

        db = (2/m) * np.sum(errors)

        # 更新参数

        w = w - learning_rate * dw.reshape(1, -1)  # 保持w的形状为(1, n)

        b = b - learning_rate * db

        # 使用已定义的compute_mse_loss函数计算损失

        loss = compute_mse_loss(y_pred, y_train)

        loss_history.append(loss)

        # 每100次迭代打印进度

        if i % 100 == 0:

            print(f"Iteration {i}: Loss = {loss:.4f}")

    return w, b, loss_history  

def main():

    """主函数：执行线性回归实验并可视化结果"""

    try:

        # 1. 加载和准备数据

        print("=== 开始加载数据 ===")

        X_train_scaled, X_test_scaled, y_train, y_test, scaler = load_and_prepare_data("housing-data.csv")

        # 确保y_train和y_test是numpy数组格式

        y_train = y_train.values if hasattr(y_train, 'values') else y_train

        y_test = y_test.values if hasattr(y_test, 'values') else y_test

        print(f"\n数据加载完成！")

        print(f"训练集: {X_train_scaled.shape[0]}个样本, {X_train_scaled.shape[1]}个特征")

        print(f"测试集: {X_test_scaled.shape[0]}个样本")

        # 2. 设置超参数并训练模型

        print("\n=== 开始模型训练 ===")

        learning_rate = 0.01

        n_iterations = 1000

        w_optimized, b_optimized, loss_history = batch_gradient_descent(

            X_train_scaled, y_train, learning_rate, n_iterations

        )

        # 3. 打印最终权重和偏置

        print("\n=== 训练结果 ===")

        print(f"最终偏置项 (b): {b_optimized:.6f}")

        print("最终权重 (w):")

        for i, weight in enumerate(w_optimized.flatten()):

            print(f"  特征 {i+1}: {weight:.6f}")

        # 4. 在训练集和测试集上评估模型

        print("\n=== 模型评估 ===")

        # 训练集评估

        y_train_pred = predict(X_train_scaled, w_optimized, b_optimized)

        train_loss = compute_mse_loss(y_train_pred, y_train)

        print(f"训练集MSE损失: {train_loss:.6f}")

        # 测试集评估

        y_test_pred = predict(X_test_scaled, w_optimized, b_optimized)

        test_loss = compute_mse_loss(y_test_pred, y_test)

        print(f"测试集MSE损失: {test_loss:.6f}")

        # 计算R²分数

        from sklearn.metrics import r2_score

        train_r2 = r2_score(y_train, y_train_pred)

        test_r2 = r2_score(y_test, y_test_pred)

        print(f"训练集R²分数: {train_r2:.4f}")

        print(f"测试集R²分数: {test_r2:.4f}")

        # 5. 绘制损失收敛图

        plt.figure(figsize=(12, 5))

        # 子图1：损失收敛过程

        plt.subplot(1, 2, 1)

        plt.plot(loss_history, 'b-', linewidth=2, label='训练损失')

        plt.xlabel('迭代次数')

        plt.ylabel('MSE损失')

        plt.title('梯度下降损失收敛过程')

        plt.grid(True, alpha=0.3)

        plt.legend()

        # 子图2：最后100次迭代的详细视图（如果迭代次数足够多）

        if len(loss_history) > 100:

            plt.subplot(1, 2, 2)

            final_losses = loss_history[-100:]

            iterations = range(len(loss_history)-100, len(loss_history))

            plt.plot(iterations, final_losses, 'r-', linewidth=2, label='最后100次迭代')

            plt.xlabel('迭代次数')

            plt.ylabel('MSE损失')

            plt.title('损失收敛细节（最后100次迭代）')

            plt.grid(True, alpha=0.3)

            plt.legend()

        plt.tight_layout()

        plt.show()

        # 6. 预测值与真实值对比图

        plt.figure(figsize=(10, 5))

        # 训练集对比

        plt.subplot(1, 2, 1)

        plt.scatter(y_train, y_train_pred, alpha=0.6, label='训练集')

        plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2, label='完美预测线')

        plt.xlabel('真实值')

        plt.ylabel('预测值')

        plt.title('训练集: 真实值 vs 预测值')

        plt.legend()

        plt.grid(True, alpha=0.3)

        # 测试集对比

        plt.subplot(1, 2, 2)

        plt.scatter(y_test, y_test_pred, alpha=0.6, color='orange', label='测试集')

        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='完美预测线')

        plt.xlabel('真实值')

        plt.ylabel('预测值')

        plt.title('测试集: 真实值 vs 预测值')

        plt.legend()

        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        plt.show()

        # 7. 打印训练总结

        print("\n=== 训练总结 ===")

        print(f"学习率: {learning_rate}")

        print(f"总迭代次数: {n_iterations}")

        print(f"初始损失: {loss_history[0]:.6f}")

        print(f"最终损失: {loss_history[-1]:.6f}")

        print(f"损失减少: {((loss_history[0] - loss_history[-1]) / loss_history[0] * 100):.2f}%")

        return w_optimized, b_optimized, loss_history, test_loss

    except FileNotFoundError:

        print("错误: 未找到数据文件 'housing-data.csv'")

        print("请确保数据文件存在于当前目录，或修改文件路径")

        return None, None, None, None

    except Exception as e:

        print(f"发生错误: {e}")

        return None, None, None, None

  

# 运行实验

if __name__ == "__main__":

    w_final, b_final, losses, final_test_loss = main()

    if w_final is not None:

        print("\n 线性回归实验完成")
```

训练结果
![](./assets/file-20251109132826297.png)
![](./assets/file-20251109132851620.png)
![](./assets/file-20251109133345727.png)
### 完整API代码实现

```python
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt  # 修正导入语句

from sklearn.preprocessing import StandardScaler  # 修正导入路径

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

  

# 设置中文字体

plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']  # 多备选字体

plt.rcParams['axes.unicode_minus'] = False

  

def load_and_prepare_data(csv_file_path="housing-data.csv"):

    """

    数据加载和预处理函数

    """

    try:

        # 1. 读取数据

        data = pd.read_csv(csv_file_path)

        print(f"原始数据形状: {data.shape}")

        # 2. 处理缺失值

        print("缺失值统计:")

        print(data.isnull().sum())

        # 删除缺失值

        data_clean = data.dropna()

        print(f"清理后数据形状: {data_clean.shape}")

        # 3. 分离特征和目标

        X = data_clean.iloc[:, :-1]

        y = data_clean.iloc[:, -1]

        print(f"特征矩阵形状: {X.shape}")

        print(f"目标向量形状: {y.shape}")

        # 4. 分割数据集

        X_train, X_test, y_train, y_test = train_test_split(

            X, y, test_size=0.2, random_state=42

        )

        # 5. 特征标准化

        scaler = StandardScaler()

        X_train_scaled = scaler.fit_transform(X_train)

        X_test_scaled = scaler.transform(X_test)

        print(f"训练集形状: {X_train_scaled.shape}, 测试集形状: {X_test_scaled.shape}")

        return X_train_scaled, X_test_scaled, y_train, y_test, scaler

    except FileNotFoundError:

        print(f"错误: 未找到数据文件 '{csv_file_path}'")

        # 生成示例数据作为备选

        print("正在生成示例数据...")

        return generate_sample_data()

  

def generate_sample_data():

    """

    生成示例数据（当真实数据不可用时）[1,2,3](@ref)

    """

    np.random.seed(42)

    X = 2 * np.random.rand(100, 3)  # 3个特征

    y = 4 + 3 * X[:,0] + 2 * X[:,1] - 1 * X[:,2] + np.random.randn(100) * 0.5

    X_train, X_test, y_train, y_test = train_test_split(

        X, y, test_size=0.2, random_state=42

    )

    scaler = StandardScaler()

    X_train_scaled = scaler.fit_transform(X_train)

    X_test_scaled = scaler.transform(X_test)

    print("已使用生成的示例数据")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

  

def train_linear_regression_with_loss_history(X_train, y_train, X_val=None, y_val=None):

    """

    训练线性回归模型并记录损失历史[4,6](@ref)

    """

    # 用于记录损失历史

    train_losses = []

    val_losses = []

    # 使用随机梯度下降来模拟迭代过程

    from sklearn.linear_model import SGDRegressor

    # 创建模型

    model = SGDRegressor(

        max_iter=1,  # 每次只迭代一步

        warm_start=True,  # 允许连续拟合

        random_state=42,

        eta0=0.01  # 学习率

    )

    # 初始化模型

    model.fit(X_train, y_train)

    n_iterations = 100

    for i in range(n_iterations):

        # 继续训练一步

        model.partial_fit(X_train, y_train)

        # 计算训练损失[3](@ref)

        y_train_pred = model.predict(X_train)

        train_loss = mean_squared_error(y_train, y_train_pred)

        train_losses.append(train_loss)

        # 计算验证损失（如果提供了验证集）

        if X_val is not None and y_val is not None:

            y_val_pred = model.predict(X_val)

            val_loss = mean_squared_error(y_val, y_val_pred)

            val_losses.append(val_loss)

        # 每20次迭代打印进度

        if i % 20 == 0:

            print(f"迭代 {i}: 训练损失 = {train_loss:.4f}")

    return model, train_losses, val_losses

  

def plot_loss_curves(train_losses, val_losses=None):

    """

    绘制损失函数曲线[1,2,3](@ref)

    """

    plt.figure(figsize=(12, 5))

    # 子图1：完整的损失曲线

    plt.subplot(1, 2, 1)

    plt.plot(train_losses, 'b-', linewidth=2, label='训练损失')

    if val_losses:

        plt.plot(val_losses, 'r-', linewidth=2, label='验证损失')

    plt.xlabel('迭代次数')

    plt.ylabel('MSE损失')

    plt.title('损失函数收敛过程')

    plt.legend()

    plt.grid(True, alpha=0.3)

    # 子图2：最后50次迭代的详细视图

    plt.subplot(1, 2, 2)

    if len(train_losses) > 50:

        final_iterations = range(len(train_losses)-50, len(train_losses))

        plt.plot(final_iterations, train_losses[-50:], 'b-', linewidth=2, label='训练损失')

        if val_losses and len(val_losses) > 50:

            plt.plot(final_iterations, val_losses[-50:], 'r-', linewidth=2, label='验证损失')

    plt.xlabel('迭代次数')

    plt.ylabel('MSE损失')

    plt.title('损失收敛细节（最后50次迭代）')

    plt.legend()

    plt.grid(True, alpha=0.3)

    plt.tight_layout()

    plt.show()

  

def main():

    """主函数：执行线性回归实验并可视化结果"""

    try:

        # 1. 加载和准备数据

        print("=== 开始加载数据 ===")

        X_train_scaled, X_test_scaled, y_train, y_test, scaler = load_and_prepare_data("housing-data.csv")

        # 确保数据格式正确

        y_train = y_train.values if hasattr(y_train, 'values') else y_train

        y_test = y_test.values if hasattr(y_test, 'values') else y_test

        print(f"\n数据加载完成！")

        print(f"训练集: {X_train_scaled.shape[0]}个样本, {X_train_scaled.shape[1]}个特征")

        print(f"测试集: {X_test_scaled.shape[0]}个样本")

        # 2. 使用能够记录损失历史的方法训练模型[4](@ref)

        print("\n=== 开始模型训练（记录损失历史）===")

        model, train_losses, val_losses = train_linear_regression_with_loss_history(

            X_train_scaled, y_train, X_test_scaled, y_test

        )

        # 3. 打印模型参数

        print("\n=== 训练结果 ===")

        print(f"最终偏置项 (intercept_): {model.intercept_[0]:.6f}")

        print("最终权重 (coef_):")

        for i, weight in enumerate(model.coef_):

            print(f"  特征 {i+1}: {weight:.6f}")

        # 4. 模型评估

        print("\n=== 模型评估 ===")

        y_train_pred = model.predict(X_train_scaled)

        y_test_pred = model.predict(X_test_scaled)

        train_mse = mean_squared_error(y_train, y_train_pred)

        test_mse = mean_squared_error(y_test, y_test_pred)

        train_r2 = r2_score(y_train, y_train_pred)

        test_r2 = r2_score(y_test, y_test_pred)

        print(f"训练集MSE损失: {train_mse:.6f}")

        print(f"测试集MSE损失: {test_mse:.6f}")

        print(f"训练集R²分数: {train_r2:.4f}")

        print(f"测试集R²分数: {test_r2:.4f}")

        # 5. 绘制损失函数曲线[1,3](@ref)

        print("\n=== 绘制损失曲线 ===")

        plot_loss_curves(train_losses, val_losses)

        # 6. 绘制预测结果对比图

        plt.figure(figsize=(12, 5))

        # 训练集对比

        plt.subplot(1, 2, 1)

        plt.scatter(y_train, y_train_pred, alpha=0.6, label='训练集')

        perfect_line = [min(y_train), max(y_train)]

        plt.plot(perfect_line, perfect_line, 'r--', lw=2, label='完美预测线')

        plt.xlabel('真实值')

        plt.ylabel('预测值')

        plt.title('训练集: 真实值 vs 预测值')

        plt.legend()

        plt.grid(True, alpha=0.3)

        # 测试集对比

        plt.subplot(1, 2, 2)

        plt.scatter(y_test, y_test_pred, alpha=0.6, color='orange', label='测试集')

        perfect_line_test = [min(y_test), max(y_test)]

        plt.plot(perfect_line_test, perfect_line_test, 'r--', lw=2, label='完美预测线')

        plt.xlabel('真实值')

        plt.ylabel('预测值')

        plt.title('测试集: 真实值 vs 预测值')

        plt.legend()

        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        plt.show()

        # 7. 训练总结

        print("\n=== 训练总结 ===")

        print(f"总迭代次数: {len(train_losses)}")

        print(f"初始训练损失: {train_losses[0]:.6f}")

        print(f"最终训练损失: {train_losses[-1]:.6f}")

        if val_losses:

            print(f"初始验证损失: {val_losses[0]:.6f}")

            print(f"最终验证损失: {val_losses[-1]:.6f}")

        loss_reduction = ((train_losses[0] - train_losses[-1]) / train_losses[0]) * 100

        print(f"训练损失减少: {loss_reduction:.2f}%")

        return model, train_losses, val_losses, test_mse

    except Exception as e:

        print(f"发生错误: {e}")

        import traceback

        traceback.print_exc()

        return None, None, None, None

  

# 运行实验

if __name__ == "__main__":

    model, train_losses, val_losses, test_loss = main()

    if model is not None:

        print("\n 线性回归实验完成！")
```

## 逻辑回归实验

逻辑回归 = 线性函数 + 概率映射函数 + 概率-分类规则

### 完整手写代码

```python
import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.datasets import load_breast_cancer

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

  

# 设置中文字体和图形样式

plt.rcParams['font.sans-serif'] = ['SimHei']

plt.rcParams['axes.unicode_minus'] = False

sns.set_style("whitegrid")

  

# 加载威斯康星乳腺癌数据集[1](@ref)

cancer_data = load_breast_cancer()

X = cancer_data.data

y = cancer_data.target

feature_names = cancer_data.feature_names

target_names = cancer_data.target_names

  

print("数据集形状:", X.shape)

print("特征名称:", feature_names)

print("目标变量分布:\n", pd.Series(y).value_counts())

print("良性样本数:", sum(y == 0), "恶性样本数:", sum(y == 1))

  

# 划分训练集和测试集

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,

                                                   random_state=42, stratify=y)

  

# 数据标准化[3](@ref)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)

  

print("\n数据预处理完成!")

print("训练集形状:", X_train_scaled.shape)

print("测试集形状:", X_test_scaled.shape)

  

class ManualLogisticRegression:

    def __init__(self, learning_rate=0.01, num_iterations=1000):

        self.learning_rate = learning_rate

        self.num_iterations = num_iterations

        self.weights = None

        self.bias = None

        self.loss_history = []

    def sigmoid(self, z):

        """Sigmoid函数，将输入映射到(0,1)区间[4](@ref)"""

        # 防止数值溢出

        z = np.clip(z, -500, 500)

        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):

        """训练逻辑回归模型"""

        n_samples, n_features = X.shape

        # 初始化参数

        self.weights = np.zeros(n_features)

        self.bias = 0

        self.loss_history = []

        # 梯度下降[6](@ref)

        for i in range(self.num_iterations):

            # 前向传播

            linear_model = np.dot(X, self.weights) + self.bias

            y_pred = self.sigmoid(linear_model)

            # 计算损失（对数似然损失）

            epsilon = 1e-15

            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

            loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

            self.loss_history.append(loss)

            # 计算梯度

            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))

            db = (1 / n_samples) * np.sum(y_pred - y)

            # 更新参数

            self.weights -= self.learning_rate * dw

            self.bias -= self.learning_rate * db

            # 每100次迭代打印损失

            if i % 100 == 0:

                print(f"迭代次数 {i}, 损失: {loss:.4f}")

    def predict_proba(self, X):

        """预测概率"""

        linear_model = np.dot(X, self.weights) + self.bias

        return self.sigmoid(linear_model)

    def predict(self, X, threshold=0.5):

        """预测类别（默认阈值0.5）"""

        probabilities = self.predict_proba(X)

        return (probabilities >= threshold).astype(int)

  

# 训练手写逻辑回归模型

print("=== 手写逻辑回归模型训练开始 ===")

manual_model = ManualLogisticRegression(learning_rate=0.1, num_iterations=1000)

manual_model.fit(X_train_scaled, y_train)

  

# 预测和评估

y_pred_manual = manual_model.predict(X_test_scaled)

y_pred_proba_manual = manual_model.predict_proba(X_test_scaled)

  

# 计算准确率

accuracy_manual = accuracy_score(y_test, y_pred_manual)

print(f"\n手写模型准确率: {accuracy_manual:.4f}")

  

# 绘制损失函数下降曲线

plt.figure(figsize=(10, 6))

plt.plot(manual_model.loss_history)

plt.title('手写逻辑回归 - 损失函数下降曲线')

plt.xlabel('迭代次数')

plt.ylabel('损失值')

plt.grid(True)

plt.show()
```
![](./assets/file-20251109141652085.png)