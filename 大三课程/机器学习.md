---
title: 机器学习
categories:
  - 大三课程
tags:
  - ML
date: 2025-10-13
---

## 感知机和神经网络

## 支持向量机

### 概述

SVM
一种二分类模型
==定义在特征空间上间隔最大的线性分类器==

> [!NOTE]
> 
> 
> |特征维度|SVM (支持向量机)|感知机 (Perceptron)|
> |---|---|---|
> |​**核心目标**​|​**间隔最大化**，追求最高泛化能力|误分类最小，只要分开就行|
> |​**解的特性**​|​**唯一**​|无穷多个|
> |​**决策边界**​|位于两类支持向量“中间”的最鲁棒的超平面|任意一个能分开数据的超平面|
> |​**依赖样本**​|仅由少数**支持向量**决定|受所有误分类点影响|
> 
> 
> - ​**决策边界**​：对于一个二元分类问题，这个划分的边界就是**决策边界**。在二维空间中，它就是一条直线（例如 w1​x1​+w2​x2​+b=0），将所有点分到两侧，一侧通常判定为类别A（“是”），另一侧为类别B（“否”）。
>     
> - ​**权重与偏置**​：线性分类器的学习过程，就是寻找这个最佳决策边界的过程。它通过调整两个关键参数来实现：
>     
>     - ​**权重**​：决定了每个特征对分类结果的重要性。权重值越大，说明该特征对分类的影响越大。
>         
>     - ​**偏置**​：允许决策边界脱离坐标原点，使模型更灵活。
>         
>     
> - ​**从得分到决策**​：对于一个输入的特征向量（例如一张图片被表示成一组数字），线性分类器会为每个类别计算一个“得分”。计算公式通常是 分数=Wx+b，其中 W是权重矩阵，x是输入特征，b是偏置向量。最终，模型会将输入归到**得分最高的那个类别**。
>     
> 
>### 追求最大间隔的原因
> 
> 为什么SVM如此执着于“最大间隔”？这主要基于两个关键考量：
> 
> 1. ​**解的唯一性**​：当训练数据线性可分时，其实存在无数个超平面都能将数据完全分开（没有错误）。感知机就是找到了其中一个解就停止，所以它的解不唯一。而SVM通过间隔最大化这个附加条件，最终找到的超平面是**唯一**的。
>     
> 2. ​**最强的泛化能力**​：这是根本原因。决策边界如果离两边的样本点都“很远”，它就相当于一个缓冲带。当出现新的、未见过的数据（测试数据）时，即使它们与训练数据略有偏差，这个“很宽”的边界也能有很大概率将其正确分类。​**间隔越大，分类边界越“安全”，模型的容错能力越强，泛化性能自然就越好**。可以想象一下，一条宽阔的马路上开车不容易撞到护栏，而一条紧贴护栏的窄路上则非常危险。
>     
> 
>  ### 间隔如何度量与最大化
> 
> SVM如何量化“间隔”呢？这里涉及两个概念：
> 
> - ​**函数间隔**​：描述样本点被分类的正确性和确信度。对于一个样本点 (xi​,yi​)，其函数间隔定义为 γ^​i​=yi​(w⋅xi​+b)。其值大于0则表示分类正确，且绝对值越大，说明点离超平面越远，分类确信度越高。
>     
> - ​**几何间隔**​：函数间隔会随着 w和 b的等比例缩放而改变，这不利于衡量真正的“距离”。因此，SVM使用更稳定的**几何间隔**，即点到超平面的真实欧氏距离：γi​=∥w∥∣w⋅xi​+b∣​。SVM的目標就是找到能使所有样本中**最小几何间隔**最大的那个超平面。
>     
> 
> 为了数学上求解方便，SVM将“最大化几何间隔”问题转化为一个等价的、更易于处理的“**最小化 21​∥w∥2​**”的凸二次规划问题。这个转化是SVM能够高效求解的关键。
> 
> 支持向量的关键角色
> 
> “支持向量机”这个名字来源于其模型的一个关键特性：最终学得的决策边界**仅仅由一小部分训练样本所决定**，这些样本就被称为**支持向量**。
> 
> 支持向量就是那些**离最终决策边界最近的、最难分的样本点**，它们恰好落在“隔离带”的边界上。正是这些样本“支撑”起了整个间隔带。​**移动或删除任何非支持向量的样本，不会对最终的决策边界产生任何影响**。因为决策边界只由这些支持向量决定，这使得SVM在处理高维数据时，在一定程度上避免了“维数灾难”。
> 
> ### 应对复杂情况：软间隔与核函数
> 
> 现实中的数据往往不是理想化的：
> 
> - ​**软间隔**​：当数据近似线性可分但存在一些噪声或特异点时，SVM引入**松弛变量**​ 和**惩罚因子C**，允许个别样本被错分或进入间隔带内部。这被称为软间隔最大化，使模型在准确性和鲁棒性之间取得平衡。
>     
> - ​**核函数**​：当数据完全线性不可分时，SVM使用**核技巧**，将数据映射到更高维的特征空间，使其在这个新空间中线性可分。核函数能直接计算高维空间的内积，避免了复杂映射带来的计算成本，巧妙解决了维数灾难问题。
>     
> 


包含核技巧->实质上的非线性分类器
软间隔，硬间隔，线性可分/不可分

### 核函数和核技巧

### 线性可分支持向量机和硬间隔最大化（重要）

==提升鲁棒性，泛化能力==

- 正例or负例？

- 函数间隔

- 几何间隔

- 最大间隔分离超平面