---
title: 机器学习
categories:
  - 大三课程
tags:
  - ML
date: 2025-10-13
---

## 感知机和神经网络

## 支持向量机

### 概述

SVM
一种二分类模型
==定义在特征空间上间隔最大的线性分类器==
包含核技巧->实质上的非线性分类器

**软间隔，硬间隔，线性可分/不可分**
![](./assets/file-20251103103802286.png)

**支持向量机分类**
![](./assets/file-20251103104057103.png)

**优缺点**
- 优点：可解释，计算复杂度与维数相关度不大，可映射到高维后解决非线性分类和回归任务
- 缺点：训练时间长，==只适合小批量样本==

**核函数和核技巧(会考)**
当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间(无限维空间)时，核函数（ kernel function）表示将输入从输入空间映射到特征空间得到的特征向量之间 的内积
==**内积是一种衡量高维空间中特征向量之间相似性的基本手段**==

>​**输入空间**是原始数据所在的空间，而**特征空间**是经过处理和提炼后、能够更好地被模型所使用的数据表示所在的空间

### 线性可分支持向量机和硬间隔最大化（重要）

![](./assets/file-20251103110616341.png)
==提升鲁棒性，泛化能力==

**定义**
![](./assets/file-20251103110934916.png)
`线性映射`：`满足加法和数乘不变性：f(x+y) = f(x) + f(y), f(kx) = k f(x)`

学习一个超平面以将正负例隔开
$$
w\times x+b=0
$$
这个式子是矩阵乘法，`w`是法向量，`b`是截距，法向量指向的一侧为正类
线性可分支持向量机的解是唯一的-=="间隔最大化"==

对应的分类决策函数
$$
f(x)=sign(w^*\times x +b^*)
$$


**函数间隔**
用于表示分类的正确性和确信度
![](./assets/file-20251103142222496.png)

**几何间隔**

>选择分离超平面时，只有函数间隔还不够。因为只要成比例地改变w和b，例如 将它们改为2w和2b，超平面并没有改变。但函数间隔却成为原来的2倍。

可以对分离超平面的法向量w加某些约束，如规范化 ||w||=1，使得间隔是确定的。这时函数间隔成为几何间隔
![](./assets/file-20251103143723766.png)

**间隔最大化**

>对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为**硬间隔最大化**

**最大间隔分离超平面**

==戴帽子的是函数间隔==
![](./assets/file-20251103195723121.png)

> ### 🔍 推导过程
> 
> 推导的核心是从直观的“间隔最大化”概念出发，通过数学变换得到上述可求解的形式。
> 
> 1. ​**从几何间隔最大化出发**​
>     
>     - 支持向量机的基本想法是找到一个能正确划分数据且几何间隔最大的超平面。几何间隔 γ表示超平面与最近样本点的最小距离。
>         
>     - 直接最大化几何间隔的原始问题可表述为：
>         
>         w,bmax​γ
>         
>         s.t.yi​(∥w∥w​⋅xi​+∥w∥b​)≥γ,i=1,2,…,N
>         
>         这个约束条件保证了超平面关于所有样本点的几何间隔至少为 γ。
>         
>     
> 2. ​**引入函数间隔并建立关联**​
>     
>     - 函数间隔定义为 γ^​i​=yi​(w⋅xi​+b)。几何间隔与函数间隔的关系为 γi​=∥w∥γ^​i​​，
>         
>         数据集整体的几何间隔为 γ=∥w∥γ^​​，其中 γ^​=mini​γ^​i​是数据集的最小函数间隔。
>         
>     - 利用此关系，可将最大化几何间隔的问题改写为：
>         
>         w,bmax​∥w∥γ^​​
>         
>         s.t.yi​(w⋅xi​+b)≥γ^​,i=1,2,…,N
>         
>     
> 1. ​**标准化函数间隔以简化问题**​
>     
>     - 函数间隔 γ^​的值会随着 w和 b的等比例缩放而改变，但这不影响超平面的位置。因此，我们可以固定 γ^​为一个常数，最常用的选择是令 γ^​=1。
>         
>     - 将 γ^​=1代入，问题转化为：
>         
>         w,bmax​∥w∥1​
>         
>         s.t.yi​(w⋅xi​+b)≥1,i=1,2,…,N
>         
>     - 注意到最大化 ∥w∥1​等价于最小化 ∥w∥。为了数学处理的便利（例如求导更简单），通常进一步转化为最小化 21​∥w∥2。这个二次形式保证了优化问题是凸的，具有很好的性质。
>         

**支持向量和间隔边界**
![](./assets/file-20251103204803492.png)
![](./assets/file-20251103204813823.png)

对偶学习算法...

### 线性支持向量机和软间隔最大化

### 非线性支持向量机和核函数

### 序列最小最优化算法


## 贝叶斯分类

### 贝叶斯决策论

>一个在概率框架下实施决策的基本方法

**举例**
![](./assets/file-20251104191748989.png)
>**后验概率** ：指在获得新的证据或信息后，对某个事件发生的可能性（即概率）进行修正后得到的概率

**贝叶斯判定准则**
![](./assets/file-20251104194608593.png)
![](./assets/file-20251104195430707.png)
![](./assets/file-20251104195518721.png)
![](./assets/file-20251104200041647.png)
>联合概率分布？

### 极大似然估计
